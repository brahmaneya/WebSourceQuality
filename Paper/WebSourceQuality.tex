\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{enumitem}
\usepackage{times}
\usepackage{subfigure}
\let\proof\relax
\let\endproof\relax
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim}
\usepackage{framed}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{framed}
\usepackage[normalem]{ulem}
\usepackage[export]{adjustbox}


\usepackage[font={small,it}]{caption}

\renewcommand{\baselinestretch}{1.0}

\renewcommand*\ttdefault{cmvtt}
\usepackage[T1]{fontenc}

% \usepackage{floatrow}
% \floatsetup[table]{font=scriptsize}
% \renewcommand\FBbskip{-10pt}


\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{example}[definition]{Example}
\newcounter{prob}
\newtheorem{problem}[prob]{Problem}

\newcommand{\agp}[1]{\textcolor{green}{Aditya: #1}}
\newcommand{\mrj}[1]{\textcolor{red}{#1}}
\newcommand{\mrjdel}[1]{\textcolor{red}{\sout{#1}}}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{0pt}
      \setlength{\parsep}{2pt}
      \setlength{\topsep}{2pt}
      \setlength{\partopsep}{0pt}
    }
}
\newcommand{\stitle}[1]{\vspace{0.5em}\noindent\textbf{#1}}
\newcommand{\squishend}{\end{list}}
\newcommand{\eat}[1]{}
\newcommand{\papertext}[1]{#1}
\newcommand{\techreporttext}[1]{}

\newcommand{\calD}{\mathcal{D}\xspace}

\newenvironment{denselist}{
    \begin{list}{\small{$\bullet$}}%
    {\setlength{\itemsep}{0ex} \setlength{\topsep}{0ex}
    \setlength{\parsep}{0pt} \setlength{\itemindent}{0pt}
    \setlength{\leftmargin}{1.5em}
    \setlength{\partopsep}{0pt}}}%
    {\end{list}}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\begin{document}

\title{Web Source Evaluation}
\numberofauthors{3} 
\author{
\alignauthor
Manas Joglekar\\
       \affaddr{Stanford University}\\
%       \affaddr{353 Serra Mall}\\
%	   \affaddr{Stanford, California 94305}\\
       \email{manasrj@stanford.edu}
\alignauthor
Hector Garcia-Molina\\
       \affaddr{Stanford University}\\
%       \affaddr{353 Serra Mall}\\
%	   \affaddr{Stanford, California 94305}\\
       \email{hector@cs.stanford.edu}
\alignauthor 
Aditya Parameswaran\\
       \affaddr{University of Illinois (UIUC)}\\
%       \affaddr{Champaign, Illinois 61820}\\
       \email{adityagp@illinois.edu}
}
\maketitle

\begin{abstract}
\end{abstract}

\section{Problem Setting}
Our database consists of a single table $T$. We have a given set of sources $s_1, s_2, ... s_m$. We extract several candidate tuples for $T$ from each of these sources. Let $t_1, t_2, ... t_n$ be the candidate tuples for all the sources. Each tuple may come from multiple sources. We define the boolean variable $b_{i,j}$ to be true if tuple $t_j$ was obtained from source $s_i$. For each $t_j$, we let $T_j$ be a boolean random variable that denotes whether of not $t_j$ is a correct tuple for table $T$. 

Each source has some accuracy level, which is itself a random variable. In addition, sources are not independent. For example, two webpages on the same site are more likely to contain similar errors. Thus even if the two webpages individually have an accuracy of $0.8$ each, the probability of both webpages being correct on any individual tuple may be more than $0.8 \times 0.8 = 0.64$. Modelling arbitrary  correlation between sources can be expensive, as the number of parameters required for $m$ sources is exponential in $m$. Thus we choose a route between the two extremes of assuming independence of all sources, and allowing arbitrary correlations in sets of sources. We assume that we are given a list of sources-groups $g_1, g_2, ...$, where each group $g$ is a set of sources that are known to be related to each other. For instance, there may be a $g$ for each website, that contains all pages on that website. Or there may be a $g$ for each author, consisting of all papers written by that author. Then for each $g_k$, we define a random variable $G_k$ that represents the `shared' part of the accuracy of sources belonging to $g_k$. We will elaborate on $G_k$ later. For each source $s_i$, we have a random variable $S_i$ that denotes the `individual' component of its accuracy (individual meaning in addition to the $G_k$ parts). 

Consider any tuple $T_j$, suppose it belongs to sources $s_{i_1}, s_{i_2}, ... $ and the source-groups $g_{k_1}, g_{k_2}, ...$ (a tuple belongs to a source group if it belongs to at least one source from that source-group). Then we create one new boolean random variables per tuple-source pair, and per tuple-group pair. That is, for $T_j$, we create boolean random variables $T_{j, s_{i_1}}, T_{j, s_{i_2}}, ...$ and $T_{j,g_{k_1}}, T_{j, g_{k_2}}, ...$. $T_{j, s_{i_1}}$ tells us whether source $s_{i_1}$ committed an error on tuple $T_j$. 

Now consider the following causal model. For each tuple $t_j$, the random variable $T_j$ is first set to true with some fixed prior probability (this corresponds to whether the tuple is actually true or not). Then, each existing $T_{j, g_k}$ is set to true with probability $G_k$, independent of all other variables. Similarly, each existing $T_{j, s_i}$ is set to true with probability $S_i$. A tuple $T_j$ is actually outputted by a source $S_i$ if the value of $T_j \oplus T_{j, s_i} \oplus(\bigoplus_{k} T_{j, g_k})$ is true. Since we can observe which sources have outputted which tuples, we can use that to estimate the values of the hidden variables $T_j$, $T_{j, s_i}$, and so on, and use those to estimate $S_i$ and $G_k$, which gives us source accuracy.

In absence of other information, $T_{j, s_{i_1}}$ is correct with a probability of $S_{i_1}$ and $T_{j, g_{k_1}}$ is correct with a probability of $G_{k_1}$.
Tuple $T_j$ will be correct if 
% xor and product make sense, but resulting logits are not factorizable, into a factor part and a source part. can use mutiplicative logits instead, but doesn't seem to have a good interpretation. or can create factors for each subset of each g, (so the number of factors and parameters is now exponential in the size of g). EDIT: Can sort of factorize, by adding the new hidden boolean variables defined above. Those vars have a solo factor that reflects the accuracies of sources, or source-groups, and there is an additional factor that relates them, via XOR or AND logical expression, to the displayed value (true). 

% Important below, justification of above design choices.
% To model fact that there are many 'false' tuples, so prob(showing|false tuple) is very low, while prob(not showing|true tuple) is not that low. To reflect this, prec/recall don't seem right, as they make no reference to unshown false tuples. So using actual conditional probabilities from previous sentence instead. What if multiple sources show a tuple, doesn't that guarantee that tuple is true due to low value of prob(showing|false)? Not necessarily, because the falseness may have come from the source-group variable (assuming sources showing the tuple belong to a common source-group), and so the low probability is taken only once, not multple times. 
%Seems hard to factor the conditional probs such that the factor weights are meaningful, and have equal values for equal things (like prior prb over all tuples should correspond to some shared factor weight). compromise by just having accuracies (instead of false pos/false neg) and only applying it to candidate tuples? So here's what I do. In the background, we have the full model with lots of false tuples, low false positive prob (but reasonable product of false pos. prob and numberof false tuples), higher false neg prob. So we have a low prior on each tuple. We could do the processing on each tuple (displayed and not displayed), and we can count a source not displaying a tuple as negative evidence. But becuase of low prior prob. we can ignore non-displayed tuples (as it takes the low false pos.probability event of a tuple being displayed in order to push it up from its low prior). And because of high false neg. prob (most sources don't get most tuples), we can ignore it when a source doesn't display a tuple. Thus our full scheme, with these approximations, reduces to the practised scheme of only taking displayed tuples, giving them a low prior prob., modifying it with source accuracy which is high (its like 1 - false pos. prob(?)) for sources which display it, and ignoring sources which don't display it. 


$S_i$ is random variable $\in \left[0, 1\right]$, accuracy of $s_i$. $T_j$ is boolean random variable, truth of $t_j$. Let $P$ denote the un-normalized joint probability distribution over all random variables. If the sources were all independent, we would have:
$$\log(P(T_1, .. T_n, S_1, ... S_m)) = \Sigma_{i,j} b_{i,j}(T_j\log(S_i) + (1-T_j)\log(1-S_i))$$


\begin{comment}
Later:
Crowdsourcing with budget
Key constraints (part of tuple correlations?)
partial extraction
partial correctness.
\end{comment}


% xor model? seems elegant to compute, and accuracies don't decay as badly with multiple factors. for the factor graph, a xor model allows us to simply hae one factor weight per sources-factor, and one per source. this links correctness of sources on tuples even after their marginal accuracies are fixed. On the other hand, we could add correlations between marginal accuracies themselves (instead of having independent uniform prior for each source)
% For tuple correlations, just have factors containing tuples. All primary key correlations have same (fixed?) weight. All ER-based corrs, have same weight, and so on. 

%\balance
{\small
\bibliographystyle{abbrv}
\bibliography{WebSourceQuality}  
}

\end{document}
