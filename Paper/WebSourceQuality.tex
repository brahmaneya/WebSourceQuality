\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{enumitem}
\usepackage{times}
\usepackage{subfigure}
\let\proof\relax
\let\endproof\relax
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim}
\usepackage{framed}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{framed}
\usepackage[normalem]{ulem}
\usepackage[export]{adjustbox}


\usepackage[font={small,it}]{caption}

\renewcommand{\baselinestretch}{1.0}

\renewcommand*\ttdefault{cmvtt}
\usepackage[T1]{fontenc}

% \usepackage{floatrow}
% \floatsetup[table]{font=scriptsize}
% \renewcommand\FBbskip{-10pt}


\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{example}[definition]{Example}
\newcounter{prob}
\newtheorem{problem}[prob]{Problem}

\newcommand{\agp}[1]{\textcolor{green}{Aditya: #1}}
\newcommand{\mrj}[1]{\textcolor{red}{#1}}
\newcommand{\mrjdel}[1]{\textcolor{red}{\sout{#1}}}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{0pt}
      \setlength{\parsep}{2pt}
      \setlength{\topsep}{2pt}
      \setlength{\partopsep}{0pt}
    }
}
\newcommand{\stitle}[1]{\vspace{0.5em}\noindent\textbf{#1}}
\newcommand{\squishend}{\end{list}}
\newcommand{\eat}[1]{}
\newcommand{\papertext}[1]{#1}
\newcommand{\techreporttext}[1]{}

\newcommand{\calD}{\mathcal{D}\xspace}

\newenvironment{denselist}{
    \begin{list}{\small{$\bullet$}}%
    {\setlength{\itemsep}{0ex} \setlength{\topsep}{0ex}
    \setlength{\parsep}{0pt} \setlength{\itemindent}{0pt}
    \setlength{\leftmargin}{1.5em}
    \setlength{\partopsep}{0pt}}}%
    {\end{list}}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\begin{document}

\title{Web Source Evaluation}
\numberofauthors{3} 
\author{
\alignauthor
Manas Joglekar\\
       \affaddr{Stanford University}\\
%       \affaddr{353 Serra Mall}\\
%	   \affaddr{Stanford, California 94305}\\
       \email{manasrj@stanford.edu}
\alignauthor
Hector Garcia-Molina\\
       \affaddr{Stanford University}\\
%       \affaddr{353 Serra Mall}\\
%	   \affaddr{Stanford, California 94305}\\
       \email{hector@cs.stanford.edu}
\alignauthor 
Aditya Parameswaran\\
       \affaddr{University of Illinois (UIUC)}\\
%       \affaddr{Champaign, Illinois 61820}\\
       \email{adityagp@illinois.edu}
}
\maketitle

\begin{abstract}
\end{abstract}

\section{Problem Setting}
Our database consists of a single table $T$. We have a given set of sources $s_1, s_2, ... s_m$. We extract several candidate tuples for $T$ from each of these sources. Let $t_1, t_2, ... t_n$ be the candidate tuples for all the sources. Each tuple may come from multiple sources. We define the boolean variable $b_{i,j}$ to be true if tuple $t_j$ was obtained from source $s_i$. For each $t_j$, we let $T_j$ be a boolean random variable that denotes whether of not $t_j$ is a correct tuple for table $T$. 

Each source has some accuracy level, which is itself a random variable. In addition, sources are not independent. For example, two webpages on the same site are more likely to contain similar errors. Thus even if the two webpages individually have an accuracy of $0.8$ each, the probability of both webpages being correct on any individual tuple may be more than $0.8 \times 0.8 = 0.64$. Modelling arbitrary  correlation between sources can be expensive, as the number of parameters required for $m$ sources is exponential in $m$. Thus we choose a route between the two extremes of assuming independence of all sources, and allowing arbitrary correlations in sets of sources. We assume that we are given a list of sources-groups $g_1, g_2, ...$, where each group $g$ is a set of sources that are known to be related to each other. For instance, there may be a $g$ for each website, that contains all pages on that website. Or there may be a $g$ for each author, consisting of all papers written by that author. Then for each $g_k$, we define a random variable $G_k \in [0,1]$ that represents the `shared' part of the accuracy of sources belonging to $g_k$. We will elaborate on $G_k$ later. For each source $s_i$, we have a random variable $S_i \in [0,1]$ that denotes the `individual' component of its accuracy (individual meaning in addition to the $G_k$ parts). 

Consider any tuple $T_j$, suppose it belongs to sources $s_{i_1}, s_{i_2}, ... $ and the source-groups $g_{k_1}, g_{k_2}, ...$ (a tuple belongs to a source group if it belongs to at least one source from that source-group). Then we create one new boolean random variables per tuple-source pair, and per tuple-group pair. That is, for $T_j$, we create boolean random variables $T_{j, s_{i_1}}, T_{j, s_{i_2}}, ...$ and $T_{j,g_{k_1}}, T_{j, g_{k_2}}, ...$. $T_{j, s_{i_1}}$ tells us whether source $s_{i_1}$ committed an error on tuple $T_j$. 

Now consider the following causal model. For each tuple $t_j$, the random variable $T_j$ is first set to true with some fixed prior probability $T$ (this corresponds to whether the tuple is actually true or not). Then, each existing $T_{j, g_k}$ is set to true with probability $G_k$, independent of all other variables. Similarly, each existing $T_{j, s_i}$ is set to true with probability $S_i$. A tuple $T_j$ is actually outputted by a source $S_i$ if the value of $T_j \oplus T_{j, s_i} \oplus(\bigoplus_{k} T_{j, g_k})$ is true. Since we can observe which sources have outputted which tuples, we can use that to estimate the values of the hidden variables $T_j$, $T_{j, s_i}$, and so on, and use those to estimate $S_i$ and $G_k$, which gives us source accuracy. Instead of assigning $0$ probability to the tuple being displayed when the $\oplus$ expression above evaluates to false, we give it a small probability $\epsilon$, to make computation easier later.

In addition to knowing which tuples were displayed, we may have knowledge about relations between certain tuples. For instance, a source may contain a line of text that can be interpreted as $(a,b)$ or $(a, b')$, but not both. In that case, we want a way to represent the fact that the probabilities of the tuples $(a, b)$ and $(a, b')$ being correct are negatively correlated. Or, we may know that say $A$ is a primary key for a table with columns $A,B$. Then two tuples $(a,b)$ and $(a,b')$ are incompatible, and again their probabilities are negatively correlated. In order to represent this fact, we create a set of tuple-groups $H_1, H_2, ..$ such that of all the tuples from any group, only one is true, with high probability (the reason behind assuming high probability rather than certainty of at most one tuple being true is to make our computation easier later). We assume that the probability penalty for conflicting tuples (belonging to a common tuple-group) being true is some small constant $\epsilon$ per common tuple-group. 

Let $P_J$ denote the un-normalized joint probability distribution over all random variables. For any tuple $t_j$, let $s(t_j)$ denote the set of indices of sources that outputted $t_j$, and let $g(t_j)$ denote the set of indices of source-groups containing a source that outputted $t_j$. For any source $s_i$, let $g(s_i)$ denote the set of indices of source groups that contain $s_i$. 
Using $\overrightarrow{T}$ to denote the vector of random variables $T_1..T_n$, $\overrightarrow{S}$ for $S_1..S_m$, $\overrightarrow{G}$ for $G_1, ... G_k$, $\overrightarrow{T_S}$ for $T_{1, s_{i_1}}, ..$, $\overrightarrow{T_G}$ for $T_{1, g_{k_1}}, ..$, $P$ is given by:
\begin{align*} 
& P_J(\overrightarrow{T}, \overrightarrow{T_S}, \overrightarrow{T_G}, \overrightarrow{S}, \overrightarrow{G}) =
\\& (\prod_{j=1}^{n} P(T_j) (\prod_{i\in s(t_j)} P(T_{j, s_i})) (\prod_{k\in g(t_j)} P(T_{j, s_i})) \times 
\\& \prod_{i\in s(t_j)} \epsilon^{I(T_j \oplus T_{j, s_i} \oplus(\bigoplus_{k\in g(s_i)} T_{j, g_k})))}
\\& \prod_{l} \prod_{1 \leq j_1 < j_2 \leq n} \epsilon^{I(T_{j_1} \land T_{j_2} \land (t_{j_1}\in H_l) \land (t_{j_2} \in H_l)})
\end{align*}
where $I$ is an indicator function.

% xor and product make sense, but resulting logits are not factorizable, into a factor part and a source part. can use mutiplicative logits instead, but doesn't seem to have a good interpretation. or can create factors for each subset of each g, (so the number of factors and parameters is now exponential in the size of g). EDIT: Can sort of factorize, by adding the new hidden boolean variables defined above. Those vars have a solo factor that reflects the accuracies of sources, or source-groups, and there is an additional factor that relates them, via XOR or AND logical expression, to the displayed value (true). 

% Important below, justification of above design choices.
% To model fact that there are many 'false' tuples, so prob(showing|false tuple) is very low, while prob(not showing|true tuple) is not that low. To reflect this, prec/recall don't seem right, as they make no reference to unshown false tuples. So using actual conditional probabilities from previous sentence instead. What if multiple sources show a tuple, doesn't that guarantee that tuple is true due to low value of prob(showing|false)? Not necessarily, because the falseness may have come from the source-group variable (assuming sources showing the tuple belong to a common source-group), and so the low probability is taken only once, not multple times. 
% Seems hard to factor the conditional probs such that the factor weights are meaningful, and have equal values for equal things (like prior prb over all tuples should correspond to some shared factor weight). compromise by just having accuracies (instead of false pos/false neg) and only applying it to candidate tuples? So here's what I do. In the background, we have the full model with lots of false tuples, low false positive prob (but reasonable product of false pos. prob and numberof false tuples), higher false neg prob. So we have a low prior on each tuple. We could do the processing on each tuple (displayed and not displayed), and we can count a source not displaying a tuple as negative evidence. But becuase of low prior prob. we can ignore non-displayed tuples (as it takes the low false pos.probability event of a tuple being displayed in order to push it up from its low prior). And because of high false neg. prob (most sources don't get most tuples), we can ignore it when a source doesn't display a tuple. Thus our full scheme, with these approximations, reduces to the practised scheme of only taking displayed tuples, giving them a low prior prob., modifying it with source accuracy which is high (its like 1 - false pos. prob(?)) for sources which display it, and ignoring sources which don't display it. 

% taking only positive tuples still seems worrying. because it always seems on any tuples, all sources are going to be right or all are going to be wrong (if they have common-ish groups). So it seems always better to blame all mistakes on factors).

\section{Solution Sketch}
Our problem involves two kinds of variables we need to estimate: (i) Boolean variables, such as the truth values of tuples, and the truth values of tuple-source or tuple-group variables ($T_{j,G_k}$), and (ii) Real number variables, which are accuracy scores of sources or source-groups. For the boolean variables, we want to find the \textit{marginal probability} of them being true, which is itself a real number. 

Given a some training data (a set of true and false tuples for the table), we can solve this problem in a way similar to Deepdive~\cite{deepdive,Niu_deepdive:web-scale}. First, we explain some terms needed for the solution. Our joint probability distribution over the boolean variables as a \textit{factor graph}. A factor graph is a bipartite graph, with two types of nodes, \textit{variables} and \textit{factors}. On one side of the bipartite graph are variable nodes, each of which corresponds to a random variable whose value we wish to infer. In our case, we create a variable node for each of our boolean random variables ($T_j, T_{j, S_i}$, etc). A \textit{possible world} refers to an assignment to all the variable nodes in the graph. The other side of the bipartite graph has factor nodes. Each factor node is connected to one or more variable nodes, and represents a relation between their probabilities. For instance, if two tuples $T_{j_1}, T_{j_2}$ are mutually incompatible because of say, a primary key constraint, then we will construct a factor node that is connected to $T_{j_1}$ and $T_{j_2}$ to represent their negative correlation. Each factor node is associated with a boolean \textit{expression} involving the variable nodes connected to it, and a \textit{weight}. To continue the example above, say we have mutually incompatible tuples $T_{j_1}, T_{j_2}$, and a factor node $f$ connected to both of them. Then $f$ will be associated with the expression $T_{j_1} \land T_{j_2}$, and a large negative weight $w_f$. The interpretation of this is that in any possible world where the expression of $f$ evaluates to true, the probability of the resulting world is multiplied by $e^{w_f}$. In this case, because $w_f$ is a large negative weight, we will heavily penalize any assignment to the variable nodes that makes both $T_{j_1}$ and $T_{j_2}$ true. In general, the un-normalized probability of a possible world is obtained by multiplying the $e^{w_f}$ values for every factor whose expression evaluates to true in that world. 

For our problem, we set the variable nodes to the boolean random variables in $\overrightarrow{T}, \overrightarrow{T_S}, \overrightarrow{T_G}$. We have a factor for each of these variables, containing that variable as the expression. For instance, for $T_j$, there exists a factor that is connected to $T_j$ alone, and has expression $T_j$. We will talk about the weights of these factors later. For each tuple-group, for each pair of tuples $t_{j_1}, t_{j_2}$ in it, we create a factor connected to both tuples with the expression $T_{j_1} \land T_{j_2}$, and weight $\log(\epsilon)$. For each tuple $t_j$, we also create a factor connecting $T_j$ and all its tuple-source and tuple-source-group variables $T_{j, S_{i_1}}, .., T_{j, G_{k_1}}, ..$. The expression for this factor is $T_j \oplus T_{j, s_i} \oplus(\bigoplus_{k\in g(s_i)} T_{j, g_k}))$, and weight is $\log(\epsilon)$. Now we have the specify the weights of the factors with the single boolean variables. The weights for the factors $T_j$ correspond to the prior probability of a tuple being correct, the weight for $T_{j, S_i}$ for each $i$ corresponds to the accuracy of source $s_i$, and the weight for $T_{j, G_k}$ corresponds to the accuracy of source-group $s_k$. Since these accuracies are not known beforehand, we need to learn the weights for these factors. We will explain how the weights are learnt later. To begin with, we can initialize the weights to some random values. 

Given a factor graph, with the graph structure, expressions, and weights specified, we want to find the probability distribution of values for each variable node in the graph. Finding the exact probability distribution is intractable for several graphs, but we can estimate the probability distribution using Gibb's Sampling. That is, we start from a random possible world, and then change the value of variables in it one at a time, in accordance with the total probability distribution. If we do this for sufficiently many iterations, the probability with which we reach a possible world approaches the actual probability of that possible world according to the total probability distribution. Then by counting the fraction of sampled possible worlds in which a variable node $T$ was set to true, we get an estimate of the marginal probability of $T$ being true. 

For learning weights, we can use some training data, along with stochastic gradient descent. Unlike for most gold-standard based evaluations, we are not using only the training data for evaluating source accuracies. We may have a small amount of training data with a large amount of unlabelled data, and our procedure uses both of these to evaluate sources. 

\begin{comment}
Later:
Crowdsourcing with budget
Key constraints (part of tuple correlations?)
partial extraction
partial correctness.
\end{comment}


% xor model? seems elegant to compute, and accuracies don't decay as badly with multiple factors. for the factor graph, a xor model allows us to simply hae one factor weight per sources-factor, and one per source. this links correctness of sources on tuples even after their marginal accuracies are fixed. On the other hand, we could add correlations between marginal accuracies themselves (instead of having independent uniform prior for each source)
% For tuple correlations, just have factors containing tuples. All primary key correlations have same (fixed?) weight. All ER-based corrs, have same weight, and so on. 

%\balance
{\small
\bibliographystyle{abbrv}
\bibliography{WebSourceQuality}  
}

\end{document}
