deepdive {

  db.default {
    driver: "org.postgresql.Driver"
    url: "jdbc:postgresql://"${PGHOST}":"${PGPORT}"/"${DBNAME} # "
    user: ${PGUSER}
    password: ${PGPASSWORD}
    dbname: ${DBNAME}
    host: ${PGHOST}
    port: ${PGPORT}
  }

  # Put your variables here
  schema.variables {
    tuple_truth.is_true: Boolean
  }

  # Put your extractors here
  extraction.extractors {

    # Extractor 1: Clean output tables of all extractors
    ext_clear_table {
      style: "sql_extractor"
      sql: """
        DELETE FROM tuple_truth;
        DELETE FROM source_outputs;
        """
    }

    ext_labeled_tuple_truth {
      style: "sql_extractor"
      sql: """
        INSERT INTO tuple_truth
        SELECT book_id || '-' || author_id, book_id, author_id, is_true, NULL
        FROM labeled_tuples_input
        """
      dependencies: ["ext_clear_table"]
    }
 
    ext_unlabeled_tuple_truth {
      style: "sql_extractor"
      sql: """
        INSERT INTO tuple_truth
        SELECT ti.book_id || '-' || ti.author_id, ti.book_id, ti.author_id, NULL, NULL
        FROM tuples_input ti
        LEFT OUTER JOIN labeled_tuples_input lti
        ON ti.book_id = lti.book_id
        AND ti.author_id = lti.author_id
        WHERE lti.is_true IS NULL
        """
      dependencies: ["ext_labeled_tuple_truth"]
    }
 
    ext_source_books {
      style: "sql_extractor"
      sql: """
        INSERT INTO source_books
        SELECT source_id, book_id
        FROM source_outputs_input
        """
      dependencies: ["ext_clear_table"]
    }

    ext_source_book_authors {
      style: "sql_extractor"
      sql: """
        INSERT INTO source_book_authors
        SELECT source_id, sb.book_id, author_id
        FROM source_books sb,
             tuples_input ti
        WHERE sb.book_id = ti.book_id;
        """
      dependencies: ["ext_source_books"]
    }

    ext_source_outputs_input {
      style: "sql_extractor"
      sql: """
        INSERT INTO source_outputs
        SELECT source_id, book_id || '-' || author_id, source_id || '-' || book_id || '-' || author_id, is_true
        FROM source_outputs_input
        """
      dependencies: ["ext_clear_table"]
    }

    ext_source_negations {
      style: "sql_extractor"
      sql: """
        INSERT INTO source_outputs
        SELECT sba.source_id, sba.book_id || '-' || sba.author_id, sba.source_id || '-' || sba.book_id || '-' || sba.author_id, false
        FROM source_book_authors sba
        LEFT OUTER JOIN source_outputs_input soi
        ON sba.source_id = soi.source_id
        AND sba.book_id = soi.book_id
        AND sba.author_id = soi.author_id
        WHERE soi.is_true IS NULL
        """
      dependencies: ["ext_source_outputs_input", "ext_source_book_authors"]
    }

  }

  inference.factors: {

    # We require developers to select: 
    #   - reserved "id" column, 
    #   - variable column, 
    #   - weight dependencies,
    # for variable tables.
    f_source_output_true {
      input_query: """
        SELECT  tuple_truth.id AS "tuple_truth.id",
                tuple_truth.is_true AS "tuple_truth.is_true",
                source_outputs.source_id AS "source_outputs.source_id"
        FROM    source_outputs, 
                tuple_truth 
        WHERE   source_outputs.tuple_id = tuple_truth.tuple_id
        AND     source_outputs.is_true = true
        """
      function: "IsTrue(tuple_truth.is_true)"
      weight: "?(source_outputs.source_id)"
    }

    f_source_output_false {
      input_query: """
        SELECT  tuple_truth.id AS "tuple_truth.id",
                tuple_truth.is_true AS "tuple_truth.is_true",
                source_outputs.source_id AS "source_outputs.source_id"
        FROM    source_outputs, 
                tuple_truth 
        WHERE   source_outputs.tuple_id = tuple_truth.tuple_id
        AND     source_outputs.is_true = false
        """
      function: "IsTrue(!tuple_truth.is_true)"
      weight: "?(source_outputs.source_id)"
    }

 }     

  # # An example of how to use the last factor graph:
  # pipeline.relearn_from: ${DEEPDIVE_HOME}"/out/2014-04-19T190341/"

  # # Default is to use the full pipeline, equivalent to:
   pipeline.run: "han"
   pipeline.pipelines.han: [
     "ext_clear_table",
     "ext_labeled_tuple_truth",
     "ext_unlabeled_tuple_truth",
     "ext_source_books",
     "ext_source_book_authors",
     "ext_source_outputs_input",
     "ext_source_negations",
     "f_source_output_true",
     "f_source_output_false"
     ]
    pipeline.pipelines.pos: [
     "ext_clear_table",
     "ext_labeled_tuple_truth",
     "ext_unlabeled_tuple_truth",
     "ext_source_outputs_input",
     "f_source_output_true",
     "f_source_output_false"
     ]
   pipeline.pipelines.extractors: [
     "ext_clear_table",
     "ext_labeled_tuple_truth",
     "ext_unlabeled_tuple_truth",
     "ext_source_books",#
     "ext_source_book_authors",#
     "ext_source_outputs_input",
     "ext_source_negations",#
     ]

  # # Specify a holdout fraction to hold out randomly
   calibration.holdout_fraction: 0.2

  # A more scientific way is to hold out by sentence:
  #  calibration.holdout_query:"""
  #    DROP TABLE IF EXISTS holdout_sentence_ids CASCADE; 

  #    CREATE TABLE holdout_sentence_ids AS 
  #    SELECT sentence_id FROM sentences WHERE RANDOM() < 0.25;

  #    INSERT INTO dd_graph_variables_holdout(variable_id)
  #    SELECT id FROM has_spouse WHERE sentence_id IN
  #    (SELECT * FROM holdout_sentence_ids);
  #  """

  # You may also try tuning sampler arguments:
  sampler.sampler_args: "-l 1000 -s 1 -i 1000 --alpha 0.1 --diminish 0.99 -n 100"
  sampler.sampler_cmd: "sampler-dw-new em"

}
